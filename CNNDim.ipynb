{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import  * \n",
    "from keras.layers import  *\n",
    "from keras import backend as K\n",
    "from keras.callbacks import  *\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "dict1={}\n",
    "dict1.update(unpickle(\"C:\\\\Users\\\\dell\\\\Desktop\\\\Datasets\\\\cifar-10-batches-py\\\\data_batch_1\"))\n",
    "dict1.update(unpickle(\"C:\\\\Users\\\\dell\\\\Desktop\\\\Datasets\\\\cifar-10-batches-py\\\\data_batch_2\"))\n",
    "dict1.update(unpickle(\"C:\\\\Users\\\\dell\\\\Desktop\\\\Datasets\\\\cifar-10-batches-py\\\\data_batch_3\"))\n",
    "dict1.update(unpickle(\"C:\\\\Users\\\\dell\\\\Desktop\\\\Datasets\\\\cifar-10-batches-py\\\\data_batch_4\"))\n",
    "dict1.update(unpickle(\"C:\\\\Users\\\\dell\\\\Desktop\\\\Datasets\\\\cifar-10-batches-py\\\\data_batch_5\"))\n",
    "#print(dict1[b'data'])\n",
    "#print(dict1[b'labels'])\n",
    "X=dict1[b'data']                # shape of X is (10000,3072)\n",
    "y=np.asarray(dict1[b'labels'])  # shape of y is (10000)\n",
    "\n",
    "\n",
    "f=32\n",
    "X=X.copy()\n",
    "X_red=X[:,0:1024]\n",
    "X_green=X[:,1024:2048]\n",
    "X_blue=X[:,2048:3072]\n",
    "# print(X_red.shape)\n",
    "clf=PCA(n_components=16*16)\n",
    "X_red=clf.fit_transform(X_red)\n",
    "# print(X_red.shape)\n",
    "X_green=clf.fit_transform(X_green)\n",
    "X_blue=clf.fit_transform(X_blue)\n",
    "\n",
    "clf=RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_red,y)\n",
    "X_red=X_red[:,clf.feature_importances_.argsort()[::-1][:12*12]]\n",
    "clf.fit(X_green,y)\n",
    "X_green=X_green[:,clf.feature_importances_.argsort()[::-1][:12*12]]\n",
    "clf.fit(X_blue,y)\n",
    "X_blue=X_blue[:,clf.feature_importances_.argsort()[::-1][:12*12]]\n",
    "X_red=np.reshape(X_red,(X_red.shape[0],12,12))         #10000 red images has been reshaped into 12*12\n",
    "X_green=np.reshape(X_green,(X_green.shape[0],12,12))   #10000 green images has been reshaped into 12*12\n",
    "X_blue=np.reshape(X_blue,(X_blue.shape[0],12,12))      #10000 blue images has been reshaped into 12*12\n",
    "\n",
    "img=np.zeros((10000,12,12,3))\n",
    "img[:,:,:,0]=X_red\n",
    "img[:,:,:,1]=X_green\n",
    "img[:,:,:,2]=X_blue\n",
    "img=img.reshape((-1, 12, 12, 3))\n",
    "img.shape\n",
    "\n",
    "inputs=Input(shape=(12,12,3))\n",
    "x=Conv2D(16,(3,3), activation='relu',padding='same')(inputs)\n",
    "x=Conv2D(16,(3,3), activation='relu',padding='same')(x)\n",
    "x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Conv2D(16,(3,3), activation='relu',padding='same')(x)\n",
    "x=Conv2D(16,(3,3), activation='relu',padding='same')(x)\n",
    "x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Conv2D(16,(3,3), activation='relu',padding='same')(x)\n",
    "x=Conv2D(16,(3,3), activation='relu',padding='same')(x)\n",
    "x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Dropout(0.25)(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(10)(x)                                         #10 for number of classes \n",
    "x=Activation('softmax')(x)\n",
    "output=Activation('softmax')(x)\n",
    "model=Model([inputs], output)\n",
    "sgd=SGD(lr=0.01,decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#print(sgd)\n",
    "model.compile(optimizer=sgd,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(img,y,nb_epoch=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SwarmPackagePy\n",
    "from SwarmPackagePy import testFunctions as tf\n",
    "from SwarmPackagePy import animation, animation3D\n",
    "\n",
    "alh=SwarmPackagePy.pso(50,tf.easom_function,-10, 10, 2, 20,w=0.5, c1=1, c2=1)\n",
    "# print(alh)\n",
    "animation(alh.get_agents(), tf.easom_function, -10, 10)\n",
    "animation3D(alh.get_agents(), tf.easom_function, -10, 10)\n",
    "\n",
    "# # model.compile(optimizer=alh,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# # model.fit(img,y,nb_epoch=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
